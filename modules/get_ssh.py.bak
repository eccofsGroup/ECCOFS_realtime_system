#!/usr/bin/env python
# coding: utf-8

import os,sys
import glob
import pandas as pd
import numpy as np
from scipy.interpolate import LinearNDInterpolator, NearestNDInterpolator
from scipy.signal import medfilt2d
from netCDF4 import Dataset, num2date
import matplotlib.pyplot as plt
from math import floor, ceil
from get_altimeter_data import get_all_altimetry
from utide import reconstruct


#sys.path.append('/Users/julia/python')
#sys.path.append('/home/julia/python')
from roms_get_grid import roms_get_grid
from roms_lonlat2ij import roms_lonlat2ij
from shapiro2d import shapiro2d
from define_obs_file import define_4dvar_obs_file, obs_provenance_definition_eccofs
from add_history import add_history
from accumarrays import accum2d

def main(fconfig):
    
    grd_file =fconfig['obs']['romsobs']['gridfile']
    grd = roms_get_grid(grd_file);
    [Fi,Fj] = roms_lonlat2ij(grd);

    ref = np.datetime64('2011-01-01') 

    working_dir     = fconfig['obs']['romsobs']['SSH']['working_dir']
    prefix = fconfig['obs']['romsobs']['SSH']['prefix']
    #working_dir     = "/p/julia/ROMS/eccofs/OBS/SSH/OBS_FILES_1h_16_3kmtide"
    # ————————————————
    # TIME PARAMETERS
    # ————————————————
    dTime      = 2.0 / 24.0            # two hours
    lag        = 1.0 / 15.0 / 24.0
    b          = (1.0/24.0/lag) / 2.0
    t_epsilon  = (4.0 / 24.0 / 60.0)     # 4 minutes

    # compute days since 2011-01-01 for various reference dates
    ndays=fconfig['obs']['romsobs']['ndays']
    end_day = np.floor((np.datetime64('now', 's')- ref) / np.timedelta64(1, 'D')) 
    start_day    = end_day-ndays
    print(start_day)
    print(end_day)   

    # ————————————————
    # FIND ALL ALONG-TRACK FILES
    # ————————————————
    all_files = glob.glob(os.path.join(working_dir, f'{fconfig['obs']['romsobs']['SSH']['prefix']}*.nc'))
    all_files = [os.path.basename(f) for f in all_files]
    
    if True:
        altimeter_data = get_all_altimetry(grd, start_day,fconfig)
    else:
        altimeter_data =np.load(f"{working_dir}altimeter_data.npz")
        
    if all_files:
            # extract the numeric timestamp from filenames
            prefix = fconfig['obs']['romsobs']['SSH']['prefix']
            d = len(prefix)
            times = [int(fn[d:-3]) for fn in all_files]  # strip prefix & ".nc"
            times.sort()

            # go back 6 days in case of updates
            start = times[-1] - 6
            if (start_day> ref + pd.Timedelta(days=times[-1])):
               start_day = ref + pd.Timedelta(days=times[-1])

            #altimeter_data = np.load('altimeter_data.npz')
    
            idxs = np.where(altimeter_data['time'] >  (start_day-10))[0]
    
            for key in altimeter_data.files:
                altimeter_data[key][:] = altimeter_data[key][idxs[0]]

 

    # ---  SSHA standard‐deviation by interpolation ---
    print("Loading SSHA mean std")
    m1 = np.load(fconfig['obs']['romsobs']['SSH']['nodacfilestat'])
    lon   = m1['lon']     # 2D array
    lat   = m1['lat']
    essh  = m1['essh']

    # pick out good points
    mask = ~np.isnan(essh)
    pts  = np.column_stack((lon[mask], lat[mask]))
    vals = essh[mask]

    # natural‐neighbor ≈ linear Delaunay
    F      = LinearNDInterpolator(pts, vals)
    essh_roms = F(grd['lon_rho'], grd['lat_rho']) * grd['mask_rho_nan']

    # 11×11 median filter
    tmp = medfilt2d(essh_roms, kernel_size=11)

    # Shapiro filter (order=2)
    tmp = shapiro2d(tmp, 2)

    # keep only positive, non‐NaN values
    mask2 = (~np.isnan(tmp)) & (tmp > 0)
    pts2  = np.column_stack((grd['lon_rho'][mask2], grd['lat_rho'][mask2]))
    vals2 = tmp[mask2]
    Fstd  = NearestNDInterpolator(pts2, vals2)


    # for tidal analysis subsampling ---
    #  Open the NetCDF file
    print("Loading tidal harmonics")
    data = np.load(fconfig['obs']['romsobs']['SSH']['tidalcoeff'], allow_pickle=True)
    coef_tide = data['coef']

    # --background MDT interpolation ---
    print("Loading mean dynamic topography")
    f = np.load(fconfig['obs']['romsobs']['SSH']['woa_merc_mdt'])
    zeta_aviso = f['zeta6km_aviso'].T * grd['mask_rho_nan']
    zeta_merc  = f['zeta6km_merc'].T * grd['mask_rho_nan']
    mask3 = (zeta_aviso != 0) & (~np.isnan(zeta_aviso))
    mean_merc  = np.nanmean(zeta_merc[mask3].ravel() )
    mean_aviso = np.nanmean(zeta_aviso[mask3].ravel() )

    zeta_background = zeta_aviso + (mean_merc - mean_aviso)

    pts_bg    = np.column_stack((grd['lon_rho'][mask3], grd['lat_rho'][mask3]))
    vals_bg   = zeta_background[mask3]
    Fbackground = NearestNDInterpolator(pts_bg, vals_bg)


    # - depth & mask interpolants over entire grid ---
    all_pts = np.column_stack((grd['lon_rho'].ravel(), grd['lat_rho'].ravel()))
    Fh = LinearNDInterpolator(all_pts, grd['h'].ravel())
    Fm = NearestNDInterpolator(all_pts, grd['mask_rho'].ravel())
    #  Compute unique integer days
    all_time = np.array(altimeter_data['time'])
   # rtime = np.unique(np.floor(all_time))
    # filter to [start_day, end_day]
   # rtime = rtime[(rtime >= start_day) & (rtime <= end_day)]

    #  Grid size (if you need L, M)
    L, M = grd['mask_rho'].shape

    #  Loop over each day
    days = pd.date_range(start_day, end_day, freq='D')
    for day_pd in days:
        datestr = day_pd.strftime('%Y%m%d')
        print(f"processing day {datestr}")
        day = (day_pd - ref).days
        print(f"processing day {day}")
        # select +/- 7 days window
        # select +/- 7 days window
        ind = np.where((all_time >= day - 6) & (all_time < day + 8))[0]
        lon        = np.array(altimeter_data['lon'])[ind]
        lat        = np.array(altimeter_data['lat'])[ind]
        ssha       = np.array(altimeter_data['ssha'])[ind]
        dac        = np.array(altimeter_data['dac'])[ind]
        ssha_dac   = ssha - dac
        time       = all_time[ind]
        provenance = np.array(altimeter_data['provenance'])[ind]

        # remove NaNs in ssha_dac
        good = ~np.isnan(ssha_dac)
        lon        = lon[good]
        lat        = lat[good]
        ssha       = ssha[good]
        ssha_dac   = ssha_dac[good]
        time  = time[good]
        provenance = provenance[good]

        # compute Fstd at these points
        fstd_vals = Fstd(lon, lat)

        # baseline weights
        denom = np.abs(np.maximum(0.02, ssha))
        weights = np.minimum(0.2, fstd_vals) / denom * (1. - np.abs(day + 0.5 - time) / 7.5)

        # for large Fstd, use alternative scaling
        mask_high = fstd_vals > 0.2
        weights[mask_high] = 10 * (1. - np.abs(day + 0.5 - time[mask_high]) / 7.5)

        # ---  Define bin sizes and compute zero‐based bin indices ---
        dLat, dLon = 1.0, 1.0
        minLat, minLon = lat.min(), lon.min()

        # MATLAB’s 1 + floor(...) becomes zero‐based floor(...)
        latBin = np.floor((lat - minLat) / dLat).astype(int)
        lonBin = np.floor((lon - minLon) / dLon).astype(int)

        # --- Grid dimensions and linearized indices ---
        latDimLength = latBin.max() + 1
        lonDimLength = lonBin.max() + 1
        mat_size = (latDimLength, lonDimLength)

        # ravel_multi_index replaces sub2ind
        varInd = np.ravel_multi_index((latBin, lonBin), dims=mat_size)

        # ---  Prepare for (“accumarray”) ---
        n_bins   = varInd.max() + 1
        onesCol  = np.ones_like(varInd)

        # count per bin
        counter=accum2d(varInd, 0, onesCol, shape=(n_bins, 1), func=np.sum).ravel()
        sgood= counter>0
        
        latm=accum2d(varInd, 0, lat, shape=(n_bins, 1),func=np.mean).ravel()
        lonm=accum2d(varInd, 0, lon, shape=(n_bins, 1),func=np.mean).ravel()

        # weighted mean of SSHa‑dac
        sshm = accum2d(varInd, 0, ssha_dac * weights, shape=(n_bins, 1), func=np.sum).ravel()
        sum_w = accum2d(varInd, 0, weights, shape=(n_bins, 1), func=np.sum).ravel()
        sshm = np.divide(sshm, sum_w)
        
        # standard deviation of ssha_dac
        stdm = accum2d(varInd, 0, ssha_dac, shape=(n_bins,1), func=np.std).ravel()
     
        #  Expected std from interpolant
        estd = np.zeros_like(stdm)
        estd[sgood] = Fstd(lonm[sgood], latm[sgood])
 
        #  Mask entire bins where stdm is too large
        bad_bins = np.where(stdm > 4 * estd)[0]
        for b in bad_bins:
            ssha[varInd == b] = np.nan

        # Tag outliers in bins with >=3 points and inflated std
        outlier_bins = np.where((counter >= 3) & (stdm > 3.0 * estd))[0]
        for b in outlier_bins:
            idx = np.where(varInd == b)[0]
            iv  = np.where(np.abs(ssha_dac[idx] - sshm[b]) > 3.0 * estd[b])[0]
            ssha[idx[iv]] = np.nan
        
        # Select “good” points for the current day
        ind = np.where((np.floor(time) == day) & ~np.isnan(ssha))[0]

        # Select “good” points for the current day
        ind = np.where((np.floor(time) == day) & ~np.isnan(ssha))[0]
        if ind.size > 0:
            lon_t        = lon[ind]
            lat_t        = lat[ind]
            ssha_t       = ssha[ind]
            time_t       = time[ind]
            provenance_t = provenance[ind]
        
   
        # remove values outside of domain, masked, or near the coast (15m isobath)
        xind = Fi(lon_t, lat_t); yind = Fj(lon_t, lat_t)
        xind = np.floor(xind).astype(int); yind = np.floor(yind).astype(int)
    
        outside_mask = (np.isnan(xind) | (xind <  0) | (yind <  0) | (xind >  M - 1) | (yind >  L - 1) | (Fh(lon_t, lat_t) < 15) | (Fm(lon_t, lat_t) == 0))
        inside = ~outside_mask

        # keep only the inside points
        ssha_t = ssha_t[inside]; lon_t  = lon_t[inside]; lat_t  = lat_t[inside]; time_t = time_t[inside]
        xind   = xind[inside]; yind   = yind[inside]; provenance_t = provenance_t[inside];
    
    
        #  bin SSH in time and space
        minTime = np.min(time_t)
        timeBin = np.floor((time_t - minTime) / dTime).astype(int)

        xdimLength    = xind.max()+1
        ydimLength    = yind.max()+1
        timeDimLength = timeBin.max()+1

        # linear index of each (xbin, ybin)
        varInd = np.ravel_multi_index((xind, yind), dims=(xdimLength, ydimLength))

        # select only valid points
        mask = (~np.isnan(ssha_t)) & (~np.isnan(xind)) & (~np.isnan(yind))
        idx  = np.where(mask)[0]

        maxVarInd = varInd.max() + 1
        
        # mean latitude & longitude per (timeBin, spaceBin)
        counter=accum2d(timeBin[idx], varInd[idx], np.ones_like(idx), shape=(timeDimLength, maxVarInd), func=np.sum)
        sgood = counter>0
        lat_b=accum2d(timeBin[idx], varInd[idx], lat_t[idx], shape=(timeDimLength, maxVarInd),func=np.mean)[sgood]
        lon_b=accum2d(timeBin[idx], varInd[idx], lon_t[idx], shape=(timeDimLength, maxVarInd),func=np.mean)[sgood]

        # mean SSH per bin
        ssha_b = accum2d(timeBin[idx], varInd[idx], ssha_t[idx], shape=(timeDimLength, maxVarInd),func=np.mean)[sgood]
         
        # mean time per timeBin
        time_b1=accum2d(timeBin[idx], 0, time_t[idx], shape=(timeDimLength, 1),func=np.mean).ravel()

        # replicate across space dimension
        time_b = np.tile(time_b1[:, None], (1, maxVarInd))
        time_b = time_b[sgood]
    
        #  build integer flags for each provenance type
        prov = np.unique(provenance_t)
        Lp = prov.size

        # multipliers: 10**[0,1,2,...]
        mult = 10 ** np.arange(Lp, dtype=int)

        # flag array same shape as provenance_t
        flag = np.zeros_like(provenance_t, dtype=int)
        for ix, p in enumerate(prov):
            flag[provenance_t == p] = mult[ix]

        # accumulate flags into super‐obs using the same binning, get sum of multipliers
        flag_b   = accum2d(timeBin[idx], varInd[idx], flag[idx], shape=(timeDimLength, maxVarInd),func=np.mean)[sgood]
    
        # decode flag_b into labels
        # For each provenance level, extract its digit in flag_b
        flag_ind = np.zeros((flag_b.size, Lp), dtype=int)
        for ix, p in enumerate(prov):
            # digit at place idx is floor(flag_b/mult[idx]) % 10, capped at 1
            digit = (flag_b // mult[ix]) % 10
            flag_ind[:, ix] = p * np.minimum(1, digit)

        # identify rows where sum across prov‐levels is zero (no obs)
        no_obs = np.sum(flag_ind, axis=1) == 0

        # build string labels with concatenated digits, replacing zero‐rows with '0'
        labels = []
        for i, row in enumerate(flag_ind):
            if no_obs[i]:
                labels.append('0')
            else:
                # join nonzero digits into string
                s = ''.join(str(d) for d in row if d != 0)
                labels.append(s or '0')

        # convert labels back to integers
        label_b = np.array([int(s) for s in labels], dtype=int)

        # provenance_b: same as label_b but any label > 999 means that more than one provenance contributed, identifies as  400
        provenance_b = label_b.copy()
        provenance_b[label_b > 999] = 400
    
        xind_b = Fi(lon_b, lat_b); yind_b = Fj(lon_b, lat_b)
    
        bad = ((xind_b <  0) | (yind_b <  0) | (xind_b >  M - 1) | (yind_b >  L - 1) | np.isnan(ssha_b) |
                np.isnan(xind_b) | np.isnan(yind_b))
        good = ~bad


        alllon = lon_b[good]; alllat = lat_b[good]; allssha = ssha_b[good]; alltime = time_b[good]
        allprovenance = provenance_b[good]; alllabel = label_b[good]
        allxind = xind_b[good]; allyind = yind_b[good]
        survey_time = np.unique(alltime)
        
    
       # initialize output lists
        AllSSH        = np.empty(0,)
        AllSSHA       = np.empty(0,)
        AllLon        = np.empty(0,)
        AllLat        = np.empty(0,)
        AllX          = np.empty(0,)
        AllY          = np.empty(0,)
        AllTime       = np.empty(0,)
        AllProvenance = np.empty(0,)
        AllLabel      = np.empty(0,)
        Nobs          = np.empty(0,)
        survey_times  = np.empty(0,)

        # loop over each survey time
        for t in survey_time:
            # indices of observations at this time
            igood = np.where(alltime == t)[0]
    
            # extract SSH and find valid (non‑NaN) entries
            ssh = allssha[igood]
            lon        = alllon[igood]
            lat        = alllat[igood]
            provenance = allprovenance[igood]
            label      = alllabel[igood]
            xind       = allxind[igood]
            yind       = allyind[igood]
 
            # compute tide
            time1 = np.arange(t - b * lag, t + b * lag,lag )
            a = len(time1)
            roms_tide = np.zeros((len(lon), a))
            for ii in range(len(lon)):
                roms_tide[ii, :] = get_tide(xind[ii], yind[ii], time1, grd['mask_rho'], coef_tide)

            mask = ~np.isnan(roms_tide[:, 0])

            #  apply mask to each array
            roms_tide  = roms_tide[mask, :]; ssh = ssh[mask]; lon = lon[mask]; lat = lat[mask]
            provenance = provenance[mask]; label = label[mask]; xind = xind[mask]; yind = yind[mask]
                             
            if lon.size > 0:
                # background + SSH
                roms_background = Fbackground(lon, lat)       # shape (N,)
                total_ssh       = ssh + roms_background       # shape (N,)

                # add the tidal perturbation for each
                #    this yields an (N, a) array
                combined_ssh = total_ssh[:, None] + roms_tide      # shape (N, a)

                # build provenance_modif = –repmat(provenance',[1 a])
                prov_modif = -np.tile(provenance[:, None], (1, a))  # shape (N, a)
                #    then flip the central column back to positive (MATLAB’s b+1 → index b)
                prov_modif[:, int(a/2)] *= -1

                # append to “All…” arrays
                AllSSH        = np.concatenate([AllSSH,        combined_ssh.flatten(order='F') ])
                AllSSHA       = np.concatenate([AllSSHA,       np.tile(ssh,       a)])
                AllLon        = np.concatenate([AllLon,        np.tile(lon,       a)])
                AllLat        = np.concatenate([AllLat,        np.tile(lat,       a)])
                AllX          = np.concatenate([AllX,          np.tile(xind,      a)])
                AllY          = np.concatenate([AllY,          np.tile(yind,      a)])
                AllProvenance = np.concatenate([AllProvenance, prov_modif.flatten(order='F')])
                AllLabel      = np.concatenate([AllLabel,      np.tile(label,     a)])

                vec_times  = np.repeat(time1.T, len(lon))   
                AllTime = np.concatenate((AllTime, vec_times))
             
 
        # Sort by time
        ind = np.argsort(AllTime)
        obs_time       = AllTime[ind]; obs_lon = AllLon[ind]; obs_lat = AllLat[ind]; obs_value = AllSSH[ind]
        obs_provenance = AllProvenance[ind]; obs_label = AllLabel[ind]; obs_error = np.ones_like(obs_lon)  
        obs_Xgrid = AllX[ind]; obs_Ygrid = AllY[ind]; obs_Zgrid = np.zeros_like(obs_Xgrid)
        obs_depth  = np.zeros_like(obs_Xgrid); obs_type = np.ones_like(obs_Xgrid)

        # Variance: [ mean(sqrt(obs_error)); ones(6,1) ] → a length-7 array
        first_var = np.mean(np.sqrt(obs_error))
        obs_variance = np.concatenate(([first_var], np.ones(6)))
                             
        # Find the unique survey times and inverse mapping
        survey_time, It, Is = np.unique(obs_time, return_index=True, return_inverse=True)
        # Prepare working arrays
        time_tmp = survey_time.copy()
        time_new = np.zeros_like(survey_time)

        #  Merge “close” survey times within t_epsilon
        for j in range(len(survey_time)):
            if not np.isnan(time_tmp[j]):
                # find all survey_time within t_epsilon of survey_time[j]
                close = np.where(np.abs(time_tmp - time_tmp[j]).astype(np.float32) < t_epsilon)[0]
                # assign the master time to all close entries
                time_new[close] = time_tmp[j]
                # update obs_time for every original observation in each close group
                # report if we merged more than one
                if close.size > 1:
                    for idx in close:
                        obs_time[Is == idx] = time_new[idx]

                    merged = survey_time[close]
                    mean_time = np.mean(survey_time[close])
                    print(f"   merging surveys at times {merged} into a survey at time {mean_time}")
                    # mark these as done
                    time_tmp[close] = np.nan

        # Recompute unique survey times after merging
        survey_time, It, Is = np.unique( obs_time, return_index=True, return_inverse=True)

        # Count how many observations per survey time
        #    (Is contains the survey index for each obs_time entry)
        Nobs = np.bincount(Is, minlength=survey_time.size)
                             
        # write observational file
        OBS_out = os.path.join(working_dir, f"along_track_TotalSSH_{day:04d}.nc")

        # number of survey times
        survey = len(survey_time)
        if survey > 0:
            define_4dvar_obs_file(OBS_out,survey,obs_provenance_definition_eccofs())
        ds = Dataset(OBS_out, 'a')
        ds.history = "Total SSH ( gdr anomaly + ROMS background + ROMS tide)  "
                         
        # history string
        addhistory = (
        "Prepared by Julia Levin. "
        "(1) SSH anomaly extracted from RADS by Kenneth Fairchild "
        "and processed using script ExtractSSHFromRADSFiles2009.m. "
        "(2) ROMS Background comes from is4dvar fit to RU climatology. "
        "(3) ROMS tide comes from harmonic analysis of forward run."
        ) 
        add_history(OBS_out, addhistory)

        # write fields to observation file
        ds.variables['spherical'][0] = 'T'
        ds.variables['Nobs'][:] = Nobs        
        ds.variables['survey_time'][:] = survey_time 
        ds.variables['obs_variance'][:]   = obs_variance
        ds.variables['obs_type'    ][:]   = obs_type
        ds.variables['obs_time'    ][:]   = obs_time
        ds.variables['obs_Xgrid'   ][:]   = obs_Xgrid
        ds.variables['obs_Ygrid'   ][:]   = obs_Ygrid
        ds.variables['obs_Zgrid'   ][:]   = obs_Zgrid
        ds.variables['obs_depth'   ][:]   = obs_depth
        ds.variables['depth'       ][:]   = obs_depth
        ds.variables['obs_error'   ][:]   = obs_error
        ds.variables['obs_value'   ][:]   = obs_value
        ds.variables['obs_lat'     ][:]   = obs_lat
        ds.variables['obs_lon'     ][:]   = obs_lon
        ds.variables['obs_provenance'][:]  = obs_provenance
        ds.variables['obs_label'   ][:]   = obs_label
        ds.close()
        
#--------------------------------------------------------------------------
def get_tide(inx, iny, time1, mask, coef):
#    """
#    Compute the tide at a fractional grid point (inx, iny) over a vector of times.

#    Parameters
#    ----------
#    inx, iny : float
#        Fractional grid‐indices (1-based in MATLAB; here we treat them as 0-based).
#    times : array‑like of datetime.datetime
#        The time vector at which to predict the tide.
#    ctide : object
#        Must have attributes:
#        - name : list of constituent names
#        - freq : array of constituent frequencies
#        - tidecon : ndarray, shape (nconst, 2, ny, nx)
#        - lat : ndarray, shape (ny, nx)

#    Returns
#    -------
#    output : ndarray, shape (len(times),)
#        The spatially‐averaged tide prediction.
#    """
    # 1) determine the integer grid corners
    ix0, ix1 = int(floor(inx)), int(ceil(inx))
    iy0, iy1 = int(floor(iny)), int(ceil(iny))

    # 2) pre‐allocate tide array: time × 2 × 2
    times1 = num2date(time1, units='days since 2011-01-01 00:00:00', calendar = 'gregorian',
        only_use_cftime_datetimes=True)
    tide = np.full((len(times1), 2, 2), np.nan)
    # 3) loop over the four surrounding grid points
    for ry, iy in enumerate((iy0, iy1)):       # ry = 0 or 1
        for rx, ix in enumerate((ix0, ix1)):   # rx = 0 or 1
            # compute harmonic coefficients
            if mask[iy,ix]:
                tmp = reconstruct(times1, coef[iy,ix], verbose=False)
                tide[:,ry,rx] = tmp.h  #reconstruct(times1, coef, verbose=False)

 
    # 4) average across the 2×2 spatial box and return a 1D array
    #    (matches MATLAB’s mean(tide(:,:),2))
    return tide.mean(axis=(1, 2))
#-----------------------------------------------------------------------

# if __name__ == "__main__":
#     get_ssh()


